{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet\n",
    "from sklearn.metrics import accuracy_score,root_mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r\"C:\\Users\\Tippu\\Downloads\\concrete+compressive+strength\\Concrete_Data.xls\")\n",
    "data.head(5)\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() #all the concrete mix aggregates are in same unit \n",
    "\n",
    "#split input and output variable\n",
    "y=data['Concrete compressive strength(MPa, megapascals) ']\n",
    "x=data.drop(['Concrete compressive strength(MPa, megapascals) '],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()  # duplicates are present around 25\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum() # no missing values are present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###UNIVARIATE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "fig=px.histogram(data['Age (day)'],nbins=300)\n",
    "fig.show()\n",
    "\n",
    "fig=px.histogram(data['Concrete compressive strength(MPa, megapascals) '],nbins=100)\n",
    "fig.show()\n",
    "\n",
    "# Density Plot\n",
    "sns.kdeplot(data['Age (day)'])   #A smoothed version of the histogram that estimates the probability density function of the variable.\n",
    "plt.title('Density Plot of Age Variable')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#histogram\n",
    "for col in data.columns:\n",
    "    sns.kdeplot(data[col])   #A smoothed version of the histogram that estimates the probability density function of the variable.\n",
    "    plt.title(f'Density Plot of {col} Variable')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1,len(data.columns), figsize=(10, 6))  # 2x2 grid of plots\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the column names for easy reference\n",
    "columns = data.columns\n",
    "\n",
    "# Plot each column in its own subplot\n",
    "for ax, column in zip(axes, columns):\n",
    "    ax.boxplot(data[column])\n",
    "    \n",
    "    ax.set_ylabel(column)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.box(data['Age (day)'])\n",
    "fig.show()\n",
    "\n",
    "fig=px.box(data['Concrete compressive strength(MPa, megapascals) '])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the skewness is positive for the variables except coarse and fine aggregate which have negative skewness\n",
    "#Positive Skewness: A distribution of income where most people earn below the average income but a few earn significantly more, pulling the mean to the right.\n",
    "#Negative Skewness: A distribution of test scores where most students score above the average, but a few score very low, pulling the mean to the left.\n",
    "\n",
    "\n",
    "#Excess Kurtosis = 0: Indicates a normal distribution.\n",
    "# Positive Excess Kurtosis: Indicates a distribution with heavier tails and a sharper peak than the normal distribution (leptokurtic). This means more data points are in the tails or peak of the distribution.\n",
    "# Negative Excess Kurtosis: Indicates a distribution with lighter tails and a flatter peak than the normal distribution (platykurtic). This means fewer data points are in the tails or peak of the distribution.\n",
    "\n",
    "# Example:\n",
    "\n",
    "#Positive Kurtosis: A distribution of extreme stock returns where there are more extreme values (both high and low) than in a normal distribution.\n",
    "#Negative Kurtosis: A distribution of heights where almost all values are clustered around the mean with very few extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    skewness=data[col].skew()\n",
    "    kurtosis=data[col].kurt()\n",
    "    print(f\" The {col} has \\nskewness:{skewness}\\n kurtosis:{kurtosis}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plot(data,dist='norm'):\n",
    "    stats.probplot(data,dist=dist,plot=plt)\n",
    "    plt.title(f'Q-Q Plot against {dist.capitalize()} Distribution ')\n",
    "    plt.show()\n",
    "\n",
    "#Interpreting the Q-Q Plot\n",
    "#Points on the Line: If the points lie approximately on the reference line (usually a 45-degree line), the data is likely normally distributed.\n",
    "#S-Shaped Curve: If the points form an S-shaped curve, the data may have heavier tails than a normal distribution (leptokurtic) or lighter tails (platykurtic).\n",
    "#Systematic Deviations: Deviations from the line, especially if systematic, indicate that the data may not follow the theoretical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(data['Cement (component 1)(kg in a m^3 mixture)']) # qq plot \n",
    "for col in data.columns:   # closely the data falls on the line with a slight deviation \n",
    "    qq_plot(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##BIVARIATE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data=data.corr()\n",
    "corr_data\n",
    "# flyash,blast furnance slag has a negative correlation which make sense as it is added to reduce cement content also to increase strength and reduce carbon footprints\n",
    "# water and superplasticizer has a negative  medium corelation\n",
    "\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "def scatt_plot(data1,data2):\n",
    "    plt.scatter(data1,data2)\n",
    "    plt.title('Scatter Plot of Variable1 vs Variable2')\n",
    "    plt.xlabel(f'Variable1')\n",
    "    plt.ylabel(f'Variable2')\n",
    "    plt.show()\n",
    "    \n",
    "scatt_plot(data['Cement (component 1)(kg in a m^3 mixture)'],data['Blast Furnace Slag (component 2)(kg in a m^3 mixture)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plot   #To visualize pairwise relationships between multiple variables in a grid format.\n",
    "sns.pairplot(data[['Cement (component 1)(kg in a m^3 mixture)', 'Blast Furnace Slag (component 2)(kg in a m^3 mixture)','Fly Ash (component 3)(kg in a m^3 mixture)','Water  (component 4)(kg in a m^3 mixture)','Superplasticizer (component 5)(kg in a m^3 mixture)','Concrete compressive strength(MPa, megapascals) ']])  \n",
    "plt.title('Pair Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=data[data['Age (day)']==365]   #14\n",
    "k_270=data[data['Age (day)']==270] #13\n",
    "k1=data[(data['Age (day)'] > 180) & (data['Age (day)'] <=365) ] #33\n",
    "k1.shape\n",
    "k.describe()\n",
    "k1.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data\n",
    "inst1=StandardScaler()\n",
    "scaled_data=pd.DataFrame(inst1.fit_transform(x),columns=x.columns)\n",
    "\n",
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(scaled_data,y,test_size=0.2,shuffle=True,random_state=42)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model-1\n",
    "#linear regression\n",
    "# add a const term \n",
    "x1=sm.add_constant(x_train)\n",
    "model_1a=sm.OLS(y_train,x1).fit()   # model is not performing well with adjusted r2=0.607\n",
    "model_1a.summary()\n",
    "model_1a.summary2()\n",
    "\n",
    "#predict the test set\n",
    "x2=sm.add_constant(x_test)\n",
    "predict_1a=pd.DataFrame(model_1a.predict(x2),columns=['predicted']).set_index(x_test.index)\n",
    "predict_1a\n",
    "\n",
    "root_mean_squared_error(predict_1a,y_test) #9.796\n",
    "r2_score(predict_1a,y_test) #0.423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=predict_1a['predicted']-y_test\n",
    "fig=px.scatter(error,predict_1a['predicted'])\n",
    "fig.show()\n",
    "#the plot shows a random scatter, this suggests that the model's assumptions are valid. \n",
    "# However, if you see patterns (like a funnel shape), it might indicate issues such as heteroscedasticity (non-constant variance)\n",
    "\n",
    "\n",
    "fig=px.histogram(error,nbins=10)\n",
    "fig.show()\n",
    "#  As residuals are approximately normally distributed ,it supports the validity of the regression model's assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression(L2 NORM)\n",
    "r=Ridge()\n",
    "param_grid={'alpha':np.logspace(-4,4,50)}\n",
    "grid_search=GridSearchCV(r,param_grid=param_grid,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "# Best model\n",
    "best_ridge = grid_search.best_estimator_\n",
    "alpha_value= grid_search.best_params_['alpha']\n",
    "print(\"Best alpha:\", grid_search.best_params_['alpha'])\n",
    "print(\"Best model coefficients:\", best_ridge.coef_)\n",
    "\n",
    "pred=best_ridge.predict(x_test)\n",
    "r2_score(y_test,pred)#62.756\n",
    "\n",
    "#NOTE:\n",
    "#It helps to prevent overfitting by shrinking the regression coefficients.and a term λ∥β∥2 is aded to cost fn.\n",
    "#A larger λ means more shrinkage of the coefficients towards zero.\n",
    "# ridge regression shrinks the coefficients and helps mitigate issues of multicollinearity and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso regression(l1 norm)\n",
    "l=Lasso()\n",
    "param_grid={'alpha':np.logspace(-4,4,50)}\n",
    "grid_search1=GridSearchCV(l,param_grid=param_grid,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_search1.fit(x_train,y_train)\n",
    "\n",
    "# Best model\n",
    "best_lasso1 = grid_search1.best_estimator_\n",
    "alpha_value1= grid_search1.best_params_['alpha'] #0.0001\n",
    "print(\"Best alpha:\", grid_search1.best_params_['alpha'])\n",
    "print(\"Best model coefficients:\", best_lasso1.coef_)\n",
    "\n",
    "pred1=best_lasso1.predict(x_test)\n",
    "r2_score(y_test,pred1) #62.75\n",
    "\n",
    "#NOTE:\n",
    "\n",
    "# a penalty equivalent to the absolute value of the magnitude of coefficients(λ∥β∥).\n",
    "# It be useful in scenarios where we need to handle multicollinearity, reduce model complexity, or select variables.\n",
    "#The L1 norm tends to shrink some coefficients exactly to zero when the regularization parameter λ is sufficiently large. \n",
    "# This results in a sparse model, where only a subset of the original variables are retained, leading to variable selection.\n",
    "# The addition of the L1 penalty term introduces bias into the estimates of the coefficients, which can help reduce the variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regression\n",
    "e=ElasticNet()\n",
    "param_grid={'alpha':np.logspace(-4,4,50),'l1_ratio':np.random.uniform(0,1,15)}\n",
    "grid_search2=GridSearchCV(e,param_grid=param_grid,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_search2.fit(x_train,y_train)\n",
    "\n",
    "best_elastic_values=grid_search2.best_estimator_\n",
    "print(\"Best model coefficients:\", best_elastic_values.coef_)\n",
    "\n",
    "pred_elastic_test=best_elastic_values.predict(x_test)\n",
    "r2_score(y_test,pred_elastic_test) #62.75\n",
    "pred_elastic_train=best_elastic_values.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode1-2\n",
    "instance_2a=KNeighborsRegressor()\n",
    "model_2a=instance_2a.fit(x_train,y_train)\n",
    "predict_2a=pd.DataFrame(model_2a.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_2a\n",
    "\n",
    "root_mean_squared_error(predict_2a,y_test) #8.65\n",
    "r2_score(predict_2a,y_test) #0.552\n",
    "#  KNN is a non-parametric method and does not produce parameters (like coefficients) that can be summarized statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper tuning of decision tree\n",
    "param_grid_knn={'n_neighbors':[5,10,15,20],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "\n",
    "grid_search_knn=GridSearchCV(instance_2a,param_grid=param_grid_knn,scoring='neg_root_mean_squared_error',cv=3,n_jobs=-1)\n",
    "grid_search_knn.fit(x_train,y_train)\n",
    "\n",
    "best_param_knn=grid_search_knn.best_estimator_ \n",
    "\n",
    "predict_knn_train=pd.DataFrame(best_param_knn.predict(x_train),columns=['predicted']).set_index(x_train.index)\n",
    "predict_knn_train\n",
    "\n",
    "#on test data\n",
    "predict_knn_test=pd.DataFrame(best_param_knn.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "\n",
    "r2_score(predict_knn_test,y_test) #0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode1-3\n",
    "instance_3a=DecisionTreeRegressor(random_state=0)\n",
    "model_3a=instance_3a.fit(x_train,y_train)\n",
    "predict_3a=pd.DataFrame(model_3a.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_3a\n",
    "\n",
    "\n",
    "root_mean_squared_error(predict_3a,y_test) #6.92\n",
    "r2_score(predict_3a,y_test) #0.7876\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper tuning of decision tree\n",
    "param_grid_dt={'criterion':['squared_error', 'friedman_mse','absolute_error'],\n",
    "            'splitter':['best', 'random'],\n",
    "            'max_depth':[None,5,10,15],\n",
    "            'min_samples_split':[2,3,5,10,15,20],\n",
    "            'min_samples_leaf':[1,2,3,5,10,15],\n",
    "            'max_features':['sqrt', 'log2'],\n",
    "            'random_state':[0,42,125]}\n",
    "\n",
    "grid_search_dt=GridSearchCV(instance_3a,param_grid=param_grid_dt,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_search_dt.fit(x_train,y_train)\n",
    "\n",
    "best_param=grid_search_dt.best_estimator_\n",
    "\n",
    "#prediction\n",
    "predict_3b=pd.DataFrame(best_param.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_3b\n",
    "root_mean_squared_error(predict_3b,y_test)#6.8859\n",
    "r2_score(predict_3b,y_test) #0.7794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#calculate the cost complexity pruning path, which provides the effective alphas and the corresponding total leaf impurities for the subtrees\n",
    "# formed by pruning.\n",
    "#Computing Cost Complexity Pruning Path gives us a series of trees pruned at different levels of complexity, controlled by a parameter called ccp_alpha.\n",
    "#ccp_alphas: List of values of the complexity parameter alpha.;As ccp_alpha increases, more of the tree is pruned, resulting in a simpler tree.\n",
    "#impurities: Total impurity of the leaves of the pruned trees.\n",
    "\n",
    "#This function calculates a series of nested subtrees by incrementally pruning the tree based on a cost complexity measure.   \n",
    "\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Get the pruning path\n",
    "path = model_3a.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Plot the total impurity vs effective alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle='steps-post')\n",
    "plt.xlabel(\"Effective Alpha\")\n",
    "plt.ylabel(\"Total Impurity\")\n",
    "plt.title(\"Total Impurity vs Effective Alpha\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train decision trees for each alpha value in the pruning path:\n",
    "pruned_trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    pruned_tree = DecisionTreeRegressor(ccp_alpha=ccp_alpha)\n",
    "    pruned_tree.fit(x_train, y_train)\n",
    "    pruned_trees.append(pruned_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each pruned tree on the validation set\n",
    "train_scores = [tree.score(x_train, y_train) for tree in pruned_trees]\n",
    "test_scores = [tree.score(x_test, y_test) for tree in pruned_trees]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, train_scores, label='Train Score', marker='o')\n",
    "plt.plot(ccp_alphas, test_scores, label='Test Score', marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.title('Cost Complexity Pruning')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "#The score() method evaluates the performance of a ML model by calculating how well it predicts the target values based on the provided feature data.\n",
    "# Specifically, in the context of a decision tree (or any supervised learning model),it typically measures accuracy. Here’s a breakdown:\n",
    "\n",
    "#Accuracy: For classification tasks, score() returns the accuracy of the model, which is the ratio of correctly predicted observations to the total observations.\n",
    "# It's calculated as: Accuracy=Number of correct predictions/ Total number of predictions\n",
    "\n",
    "#R² Score: For regression tasks, the method may return the R² (coefficient of determination) score, which indicates how well the model's predictions \n",
    "# match the actual data. The R² score varies from 0 to 1, where 1 indicates perfec\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the pruned tree with the highest test score\n",
    "best_tree_index = test_scores.index(max(test_scores))\n",
    "best_tree = pruned_trees[best_tree_index]\n",
    "Best_alpha=ccp_alphas[best_tree_index]\n",
    "# Evaluate the best tree\n",
    "print(f\"Best alpha: {ccp_alphas[best_tree_index]}\")\n",
    "print(f\"Train score: {train_scores[best_tree_index]}\")\n",
    "print(f\"Test score: {test_scores[best_tree_index]}\")\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt1=DecisionTreeRegressor(random_state=0,ccp_alpha=0.01507668645580644)\n",
    "bt=bt1.fit(x_train,y_train)\n",
    "#prediction\n",
    "predict_3b1=pd.DataFrame(bt.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_3b1\n",
    "root_mean_squared_error(predict_3b1,y_test)#6.8864\n",
    "r2_score(predict_3b1,y_test) #0.7823\n",
    "\n",
    "#########################################################\n",
    "bt2=DecisionTreeRegressor(criterion='squared_error',random_state=0,ccp_alpha=0.01507668645580644,max_features='sqrt',max_depth=10,min_samples_split=5,min_samples_leaf=3)\n",
    "bt2=bt2.fit(x_train,y_train)\n",
    "#prediction\n",
    "predict_3b2=pd.DataFrame(bt2.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_3b2\n",
    "root_mean_squared_error(predict_3b2,y_test)#6.95\n",
    "r2_score(predict_3b2,y_test) #0.7642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper tuning of decision tree\n",
    "import numpy as np\n",
    "param_grid_dt1={'ccp_alpha': np.linspace(0, 0.1, num=50)\n",
    "}\n",
    "\n",
    "grid_search_dt1=GridSearchCV(best_tree,param_grid=param_grid_dt1,scoring='neg_root_mean_squared_error',cv=3,n_jobs=-1)\n",
    "grid_search_dt1.fit(x_train,y_train)\n",
    "\n",
    "best_param1=grid_search_dt1.best_estimator_  #best_param1 #ccp_alpha=0.07142857142857144\n",
    "\n",
    "#prediction\n",
    "predict_dt_test=pd.DataFrame(best_param1.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_dt_test\n",
    "root_mean_squared_error(predict_dt_test,y_test)#6.8991\n",
    "r2_score(predict_dt_test,y_test) #0.78587\n",
    "\n",
    "#prediction\n",
    "predict_dt_train=pd.DataFrame(best_param1.predict(x_train),columns=['predicted']).set_index(x_train.index)\n",
    "predict_dt_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode1-4\n",
    "instance_4a=RandomForestRegressor(oob_score=True,verbose=True,random_state=42)\n",
    "model_4a=instance_4a.fit(x_train,y_train)\n",
    "predict_4a=pd.DataFrame(model_4a.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_4a\n",
    "\n",
    "root_mean_squared_error(predict_4a,y_test) #5.624\n",
    "r2_score(predict_4a,y_test) #0.8536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper tuning of random forest\n",
    "param_grid_rdt={'ccp_alpha': np.linspace(0, 0.8, num=50)}\n",
    "\n",
    "grid_search_rdt=GridSearchCV(instance_4a,param_grid=param_grid_rdt,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_search_rdt.fit(x_train,y_train)\n",
    "\n",
    "best_param_rdt=grid_search_rdt.best_estimator_\n",
    "\n",
    "#prediction\n",
    "predict_4b=pd.DataFrame(best_param_rdt.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_4b\n",
    "root_mean_squared_error(predict_4b,y_test)#5.6\n",
    "r2_score(predict_4b,y_test) #0.8536\n",
    "\n",
    "\n",
    "predict_rdt_train=pd.DataFrame(best_param_rdt.predict(x_train),columns=['predicted']).set_index(x_train.index)\n",
    "predict_rdt_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model-5\n",
    "#xgb boost\n",
    "instance_5a=XGBRegressor()\n",
    "model_5a=instance_5a.fit(x_train,y_train)\n",
    "predict_5a=pd.DataFrame(model_5a.predict(x_test),columns=['predicted']).set_index(x_test.index)\n",
    "predict_5a\n",
    "\n",
    "root_mean_squared_error(predict_5a,y_test) #4.452\n",
    "r2_score(predict_5a,y_test) #0.9151\n",
    "\n",
    "predict_xg_train=pd.DataFrame(model_5a.predict(x_train),columns=['predicted']).set_index(x_train.index)\n",
    "predict_xg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine predictions into a new feature matrix for meta model\n",
    "x_meta_train=np.column_stack((pred_elastic_train,predict_knn_train,predict_dt_train,predict_rdt_train,predict_xg_train))\n",
    "x_meta_train\n",
    "\n",
    "# Combine test predictions\n",
    "x_meta_test=np.column_stack((pred_elastic_test,predict_knn_test,predict_dt_test,predict_4b,predict_5a))\n",
    "x_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model=Ridge()\n",
    "meta_model.fit(x_meta_train,y_train)\n",
    "\n",
    "# Predict with meta-model\n",
    "y_pred_ = meta_model.predict(x_meta_test)\n",
    "\n",
    "\n",
    "rmse_ =root_mean_squared_error(y_pred_,y_test) \n",
    "print(f'Mean Squared Error of Stacking Regressor: {rmse_:.2f}')#7.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model1=XGBRegressor()\n",
    "meta_model1.fit(x_meta_train,y_train)\n",
    "\n",
    "# Predict with meta-model\n",
    "y_pred_1 = meta_model1.predict(x_meta_test)\n",
    "\n",
    "mse_1 = root_mean_squared_error(y_test, y_pred_1)\n",
    "print(f'Mean Squared Error of Stacking Regressor: {mse_1:.2f}')#58.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
